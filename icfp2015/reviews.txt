===========================================================================
                           ICFP '15 Review #38A
---------------------------------------------------------------------------
    Paper #38: A Coq Library For Internal Verification of Running-Times
---------------------------------------------------------------------------


                      Overall merit: 4. Accept
                 Reviewer expertise: 3. Knowledgeable

                         ===== Paper summary =====

The paper presents the background theory, an approach and a tool
for annotating functions in Coq with run-time information and
proving their correctness and complexity. Everything is carefully
designed to work with Coq's Program extension and with program
extraction into OCaml, such that the run-time annotations and the
properties do not clutter the generated code.

                          ===== Evaluation =====

The strength of the paper is the clear explanation of the
approach and design choices. All major points are explained
together with an example that illustrates the issue. With the
coupling of run-time information and correctness properties the
authors are able to extract information from
invariants/preconditions for the run-time correctness proofs.
This is in particular useful for their example of Braun trees.

The paper is written in an excellent way. I only found 1 typo,
see below.

I find section 6 the weakest section of the paper. It is clear
that an approach with abstract run-times will never be able to
cover all effects when executing the program on a real machine.
It is important to list the assumptions that are necessary for
the abstraction (this is what sect. 6 partially does) and to
discuss what effects would or would not have an influence on the
results and provide proofs where possible (this is also done in
sect. 6). I am not so sure whether it makes sense to provide
figures instead of a proof (Fig 5 and Fig 6).

There is a discussion about garbage collection, but not about
compaction. I expect compaction to be at least linear in the
amount of live data. Instead of speculating about the complexity
of the garage collector, it might be better to discuss how to
link the results in this paper with complexity results for
garbage collectors (I would expect real results as future work
only here).

I don't buy the argument about the constant time of doubling and
adding 1 at the top of page 10. This is true if the bignum lib
uses base 2. With any more realistic and larger base, one needs
to shift the whole number, right?

                    ===== Comments for author(s) =====

arrow -> missing after the nested forall in the last Coq display
on page 2

page 7: Do we really need the full table? IMHO, a small,
quarter-page size table showing the essentials should suffice.

sect 6: I would suggest a list of assumptions/effects that have
an influence on the validity of the abstract complexity results.
If you do discuss garbage collection, please take compaction into
account.

Please explain your constant time argument for double and add on
page 9 for bases > 2.

===========================================================================
                           ICFP '15 Review #38B
---------------------------------------------------------------------------
    Paper #38: A Coq Library For Internal Verification of Running-Times
---------------------------------------------------------------------------


                      Overall merit: 2. Weak reject
                 Reviewer expertise: 2. Some familiarity

                         ===== Paper summary =====

The paper introduces a Coq library that incorporates a model of running time into the type of a function.

                          ===== Evaluation =====

Unfortunately, the paper does not meet the presentational standards to be expected for a paper presented and published at the ICFP conference.

For example

- the introduction does not introduce the general area, but instead starts by giving "a sense of what code using our library looks like", and soon is explaining code line by line. So, the paper gives no clear context for the work.

- More importantly, it is by no means clear what the papers *contributions* are. Is it the library, is a "Coq-to-Coq" translation function (means?), a combination of these, or something else?

- The standard of exposition is too informal in a number of places. Take, for example, the last two paragraphs of the "related work" section.
	- the first says: we've called our work "monads" but "it probably is not technically accurate to use that term" … well, that needs fixing.
	- the second says: "our code builds heavily on XXX" … well, explain how and why: again this provokes the main criticism: the paper simply doesn't clearly articulate its contribution(s).

                    ===== Comments for author(s) =====

As above.

===========================================================================
                           ICFP '15 Review #38C
---------------------------------------------------------------------------
    Paper #38: A Coq Library For Internal Verification of Running-Times
---------------------------------------------------------------------------


                      Overall merit: 2. Weak reject
                 Reviewer expertise: 3. Knowledgeable

                         ===== Paper summary =====

This paper presents a monadic programming in Coq for keeping track of
the number of computation steps in a program run. The paper includes
light-weight analysis of Set/Prop distinction in Coq's code extraction
and definitions of monad operators, which appropriately count
computation steps. The authors implemented various algorithms found in
the literature, such as balanced trees and sorting, using the proposed
monad. The paper concludes with investigation of the accuracy of costs
for primitive functions, and discusses whether computation steps
calculated by the framework is reasonable.

                          ===== Evaluation =====

The paper was a nice read, but the obtained result so far is not sufficient
enough to warrant publication in ICFP. 

I miss the motivation for the work: why do we want to count the precise
number of computation steps in a program run? Verification in a proof
assistant is effort- and cost-demanding. There should be a critical
need for correctness, which justifies the required effort and
time. (The work may have an academic value without the justification,
but it would be difficult to convince the practical use without.)

                    ===== Comments for author(s) =====

Page 3, left, Coq code.
The statement does not parse. Perhaps → is missing?

Page 3, left, Coq code.
Is n a typo for an?

Figure 2.
Why in Figure 2 the insert function increments the cost by 6
(resp. 9) whereas it does so by 1 (resp. 1) in Figure 1?

===========================================================================
                           ICFP '15 Review #38D
---------------------------------------------------------------------------
    Paper #38: A Coq Library For Internal Verification of Running-Times
---------------------------------------------------------------------------


                      Overall merit: 3. Weak accept
                 Reviewer expertise: 3. Knowledgeable

                         ===== Paper summary =====

This paper describes the design of a monad for tracking the running time of
functional programming. This monad includes a virtual clock, plus an operation
to increment that clock. (Coq's extraction mechanism ensures that that the
"virtual clock" is indeed virtual.)

The authors verified the running time of seventeen different functions using
this monad, with the assistance of a translator to automatically add "ticks"
to the clock at appropriate spots according to Rosendahl's cost semantics.

                          ===== Evaluation =====

Points in favor:

+ This paper makes the observation that the proof of the running time of a
  function sometimes requires additional correctness properties. The encoding
  provided in this paper allows those correctness properties to be defined at
  the same time as the algorithm (i.e. internal verification of running time).

Points against:

- The idea of using a parameterized monad to track the running time of
  functional programs is not new to this work. Danielsson (POPL 2008) already
  does so, though using a less expressive monad.

- It is difficult to draw conclusions from the experimental results presented
  in Section 5.

                    ===== Comments for author(s) =====

The introduction to this paper seems abrupt to me. What problem is it trying
to solve? What are the main contributions of the paper? What should I be
looking for?

Also, I would like to see a little more discussion about what Braun trees are
used for. From the description, they seem to be a form of balanced binary
tree, but unlike red black trees they do not store numbers in order.  Why were
they invented?

Figure 1 isn't complete without the obligations from "Program".  What
additional facts must be shown for this definition to be accepted by Coq?

How hard would it be to do an analysis to automatically calculate the running
time of these algorithms?

Why did you choose Rosendahl's cost semantics? Is there a connection between
this semantics and some idealized version of OCaml compilation?  Is there a way 
show that the tick-adding translation is correct? What would that take?

Section 6 is a good start, as it validates some of the primitive operations via testing. However, this work doesn't validate the entire translation. For example, the discussion of garbage collection needs to be expanded. I don't believe that most languages use a two-space copying collector, what about generational collection?


Why does insert_result include a stricter correctness condition? Is the
translation doing more than adding appropriate +1s to the code? 

Why not show the definition of the ret, inc and bind as well as their types?

Perhaps the problem discussed in Section 4 more general than this paper
describes. In particular, what about defining other monads besides those that
track running time? Would this problem show up in other contexts? Is this an
issue with internal verification and using monads to structure code in
dependently-typed languages?

Which functions in section 5 need the most expressive version of bind?

What conclusions should we draw from the table in Figure 3? Why are the line
counts from generated code included? Wouldn't line counts of the input give a
better sense of the user-facing part of the library? Is the intent to display
the ratio of code to time & correctness proof? Or give a general sense of the
scale of the case study?

What proofs are in the "other" category for proofs? Why make this division?

Were there any algorithms that you wanted to show that you could not? How did
you decide which 17 algorithms to implement?

===========================================================================
                           ICFP '15 Review #38E
---------------------------------------------------------------------------
    Paper #38: A Coq Library For Internal Verification of Running-Times
---------------------------------------------------------------------------


                      Overall merit: 1. Reject
                 Reviewer expertise: 3. Knowledgeable

                         ===== Paper summary =====

The paper's goal is to use Coq to reason about the running times of
functions. The paper uses an approach similar to Danielsson's
"Lightweight Semiformal Time Complexity Analysis", where the programs
are defined in a monad which keeps track of the number of reduction
steps used. Compared to Danielsson's paper, the main difference is
that the code that keeps track of reduction steps is erased at
runtime. The proof obligations are also kept separate from the
algorithm, whereas in Danielsson's approach they are intertwined.
The authors have verified a variety of algorithms using their approach.

                          ===== Evaluation =====

This work is closely related to Danielsson's, both papers combining
the use of dependent types with a monad to count reduction steps.
Since the related work is at first glance so similar, the paper needs
to state clearly what new contributions it makes. Unfortunately, the
paper starts right off the bat with using the library and doesn't explain
the value of the paper or put the work in context.

Reading the related work section, I gleaned that the paper has two
advantages over Danielsson's:
  1) The complexity information is erased during compilation, so that
     there is no overhead running the extracted functions.
  2) The approach can reason about a wider variety of algorithms.

The fact that all the complexity information is erased is nice. The
paper does this by encoding the complexity information using Props,
which are erased in Coq. This is a nice contribution.

As far as reasoning about a wider variety of algorithms, I am not sure
this is true. The paper claims that
   "Danielsson’s system cannot specify the running time of many of the
   Braun [tree] functions, since the size information is not available
   without the additional assumption of Braunness"
which I found hard to believe. So I tried specifying Braun tree
insertion using Danielsson's library and it went through fine!

The paper makes this claim because it allows the user to assume the
data type invariant when proving complexity bounds, while in
Danielsson's library the bound must be proved for all elements of the
data type, even those for which the invariant doesn't hold. However, I
believe this just reflects a difference in programming styles. There
are two ways to go about defining a data type with an invariant:
either define a "dumb" data type and then a separate invariant, or
bake the invariant into the constructors of the data type itself. The
paper chooses the first approach, so Braun tree insertion is defined
as a function from plain binary trees to binary trees, with a separate
proof that insertion preserves Braunness. In this setting you can only
prove that insertion has the correct complexity if you assume that the
tree you're inserting into is actually a Braun tree, and the approach
of this paper is needed. In the second approach, you define the type
of Braun trees directly, and insertion is a function from Braun trees
to Braun trees. In this setting Danielsson's approach works perfectly
fine as there is no distinction between data type and invariant.

So the contributions of this paper compared to Danielsson's are, in my
view: 1) it doesn't leave any complexity computations in the extracted
code, and 2) it works even when the programmer separates the data type
from its invariant. This is a worthwhile problem to solve, the idea is
interesting and a well-written paper on it could potentially be
suitable for ICFP.

Unfortunately, the writing and exposition are not up to scratch. The
paper simply describes the implementation of the library and how it is
used: it does not explain the design decisions and tradeoffs made, or
justify the design's correctness, or even say what the contribution of
the paper is.

The monad is defined in an unsafe way - a user with access to the
internals of the monad can "prove" any complexity they want. To make
the library safe, the monad is made abstract and only safe functions
are provided for using it. Fair enough, but this does make the design
tricky to get right. Because of the high potential for error I would
expect the paper to justify why the operations are indeed safe and why
they guarantee the correct running time (maybe even with some proofs!)
and to point out any subtleties in the design.

Instead, the paper simply defines all the monad operations in a
plausible way and doesn't comment on them further. That's not quite
convincing enough. An example of why not is the monad laws.
Take the two terms:
  m >>= \x -> f x >>= g
  (m >>= f) >>= g
These are supposed to be equivalent but, if I understand correctly,
the costs are assigned differently! In the first program, if P is the
complexity predicate then one of the things we check is
  P (f x) (cost of computing f x)
In the second program, the leftmost bind also has to account for the
cost of m so we get
  P (f x) (cost of computing f x + cost of computing m)
Probably this is a deliberate choice, but the paper needs to explore
these things if it's going to convince the reader that the design
makes sense. Examples would help a lot - for example, showing what
happens to the proof obligations when you have a sequence of binds.

I did enjoy reading the paper, and the authors seem to have made a
useful library. Unfortunately, the paper itself is not in good shape
at the moment. It needs reworking to get across the whys and hows of
the design of the library, and to explain how the paper improves on
the state of the art. It also should try to persuade the reader that
the design is correct, not just describe the implementation.

                    ===== Comments for author(s) =====

Just miscellaneous comments, I have written most things in the main review:

I think you've used a bigger font than you were supposed to!

I think there is a problem with the formalisation of the monad laws.
When you use sig_eqv to compare two monadic expressions,
  {x | exists m. P x m}
  {y | exists n. P x n}
you are comparing the two components:
  x = y
  (exists m. P x m) <-> (exists n. P y n)
but you are not checking that m and n are equal! This should be fixed
as otherwise you do not know if the monad laws preserve running time.

The proof scripts look quite brittle, referring to particular concrete
numbers. For example, the following lines are from rbt_insert.v:
  simpl (27 + 42).
  replace (69 * h + 8 + 8 + 14) with (69 * h + 30); try omega.
Obviously, very small changes to the program would invalidate the
proofs. It would be nice to have proof tactics which helped with this,
e.g., letting you forget about constant factors in the proofs.

Just an idle thought: There are quite a few "exists n. P x n" in the
paper (the definition of C, the extra argument to bind). Given that
every function is going to terminate after some number of steps, this
formula really seems to state that the data structure invariant is
true. I wonder if it would clarify the design to split P into two
pieces, one for the invariant and one for the running time.

One thing I liked about your approach is that the proofs are done
completely separately from the function definition. This is a clear
advantage over Danielsson, where you have to put proof terms in the
middle of your definitions. I would mention this!
