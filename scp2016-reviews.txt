Reviewer #1: Summary
=======

This paper introduces a Coq library for proving that a program have a
specific running time. The programmer writes the program with a monadic
structure, which is then automatically translated into a more detailed
program containing additional annotations about how much time each part of
the program takes according to a chosen cost model. The programmer predicts
the running time (as a precise function) and formulates related correctness
properties, and proof obligations can then be synthesised and discharged.
After establishing that the running time of the program is precisely
described by a particular function, that function can then be separately
proved to have some asymptotic complexity. The authors put special emphasis
on the efficiency of the extracted program, striving to make the monadic
structure, additional annotations, and proofs completely erased from the
extracted program. A substantial case study is conducted and the result
analysed. Informal arguments are also presented to show that the asymtotic
complexity results established in the case study are valid even though the
adopted cost model does not account for the costs of numeric primitive
operations (on arbitrary-precision numbers).

Overall comments
================

In general, the paper is well-written and easy to follow. Section 5 is
especially well done, gently leading the reader through the design of the
monadic types which enable clean extraction and can correctly express that
a program has a specified running time. The case study is substantial and
nicely summarised, justifying the feasibility of the approach (although the
line counts are slightly scary).

One major concern is about the need to prove precisely that the running
time is a specific function. At first, when going through the Braun tree
insertion example, I had the impression that after a program is translated
into one with running-time annotations, the programmer has to somehow
magically find a function that is precisely the running time of the
program, which sounds difficult in general. But then the last paragraph on
p13 explains that the running-time function in general follows the
structure of the program, and only in simple cases (like Braun tree
insertion) do the authors write down a closed form of the function
directly. After reading this, the approach sounds a lot more feasible.
I would suggest explaining this approach earlier in the paper. (For
example, on p5, instead of showing insert_time directly, you could say that
in general you would define insert_time recursively, following the
structure of insert, but since in this case the recurrence relation is
easily solvable, you simply choose to define insert_time in a closed form.)

(A follow-up question is whether the running-time function can also be
extracted from the program, seeing that it usually follows the latter's
structure. The Braun tree insertion example suggests that this is not
possible: We know insertion takes log time because Braun trees are
balanced, but insert assumes nothing about tree balancing. The next
question is then whether such extraction is possible for programs on trees
with *internalised* balancing constraints, and this time it sounds more
likely. This could lead to a counter-argument to your insistence on writing
"idiomatic code" — by including more (type) information in the program,
more properties about the program can be automatically deduced.)

Still, it looks like a waste of effort having to first reason about running
time precisely and then prove asymptotic results, deducing the constants
and lower order terms only to throw them away. Have you considered the
possibility of reasoning directly with big-O, like what we usually do
informally (like "this step takes O(1) time, and repeats O(log n) times, so
the whole algorithm takes O(1) * O(log n) = O(log n) time")? If yes, could
you include a discussion in the paper?

Section 8 presents only informal time complexity arguments and does not
seem to make use of the library. I imagine that the informal arguments were
supplied only after the case study was done, and it is hard to revise/redo
everything when you have over ten thousand lines of code. But still, this
is formally a change of cost model, and you claim at the end of Section 3
that your monad can support different cost semantics, so it is slightly
disappointing that you do not use the opportunity to illustrate how your
library can deal with a different (and probably more complex) cost
semantics that considers numeric primitive operations. You do mention in
Section 7, though, that you have proved in Coq that the iterative fibonacci
takes quadratic time if the running time of addition is taken into account.
If this proof is facilitated by your library, could you explain how in the
paper?

Detailed comments
=================

p3 first paragraph:
  "the sizes of two Braun trees s t" -> s and t
  "s_size <= t_size <= s_size + 1" -> t_size <= s_size <= t_size + 1
  "combining the s and t": delete "the"

p4 "since we know that the tree is not empty": It might be helpful to
explicitly mention that this is because of the antecedent Braun (bt_node j
s t) n, whose inversion forces n to be of the form s_size + t_size + 1.

p5 first line "we've", and various other uses of contractions throughout
the paper: It is usually advised not to use contractions in academic
writing. Although this may count as a stylistic issue, the contractions
does appear slightly unnaturally to me when I read through, though
(probably because people usually avoid them in papers).

p5 "it also includes a stricter correctness condition": Is this essential
for establishing the running time, or just included for showing complete
functional correctness?

p7 last paragraph: This paragraph is slightly abstract: Why is P a 0
irrelevant or wrong? I needed to go back to the Braun tree example and use
a concrete P to see that — it would be helpful to explain in terms of a
specific P here in the text.

p7 last paragraph "We know, however that": insert comma after "however"

p8 first line "for of the entire expression": delete "of"

p10 "make_array: [...] two n log n and a linear version": Italicise the two
n's.

p16 "But the Braun tree invariant tells us that lsize and rsize are either
equal, or that lsize is rsize+1" -> ... tells us either that lsize and
rsize are equal, or that ...

p16 "thus the run time of fib is bounded below by a factor of n^2": insert
'and' before "thus"

p18 "The first of category": delete "of"



Reviewer #2:
This paper describes a Coq library that provides the definition of a monad,
which can be used to count the number of "operations" performed during the
evaluation of the code. (Each function call, lookup and case analysis account
for one operation.) An auxiliary tool is provided to turn uninstrumented Coq
code into the corresponding instrumented Coq code with return values in the
monadic type and with the appropriate costs (e.g. "+= 6;"). The authors
consider a number of examples featuring interesting recursion patterns,
and they show that they are able to establish the expected asymptotic
complexity bounds.

Although the idea of such a monad, which essentially counts the "ticks" as
the program runs, seems straightforward, the technical details required to
correctly set up the bind operations of the monad turn out to be quite delicate.
It appears to be nontrivial to define a bind operator that can be successfully
used for defining and reasoning about recursive functions by well-founded
recursion in Coq. Nevertheless, the definition of "bind" proposed by the
authors appears quite clean and proves to be usable in practice.

Overall, I found the paper well written, very clear, and quite interesting.
However, I have one major reservation about it, concerning the last third
of the paper. This is the part that was not included in the conference
version of the paper, if I understand correctly.

The authors spend 7 pages argueing, on paper (i.e. not in Coq),
that for each of their examples, it is safe to ignore the execution cost
associated with the arithmetic operations on big-int numbers, either because
such big-int operations can be implemented in constant time, or because their
cost is dominated by other operations. Such arguments may be valuable in the
context of algorithmic analysis, but in the context of a formal proof I find
them fairly disappointing. (More broadly, I have seen the formal community be
very critical of "semi-formal proofs", where half of the proof is done in Coq
and the other half on paper.)

I can see several ways of fixing the issue here---and their might be other ways, too.

(1) Simply claim that your Coq analysis formally counts the number of big-int
    operations involved, acknowledging that these operations might be non-constant.

(2) Consider a concrete implementation of big-int operations (not necessarily an
    optimized one, e.g. list of bits would be fine), and define within your
    framework the "succ", "add" and "div2" and other operations, with a monadic
    type reflecting the cost of these operations; then, you should be able
    to significantly strengthen the claim of your paper, replacing many of the
    paper proofs with corresponding Coq proofs. (Moreover, you would also show
    on the way that your monad supports modular reasoning.)

(3) If the above solution is too much work to apply to all examples, you could
    apply the proposal (2) to half of your examples, and apply te proposal (1)
    to the other half. Or even drop a few of your many examples. In any case,
    the paper would become much stronger, imo.


Besides, I have a bunch of minor comments listed below.


- p4, line 34, missing the word "how".

- p4, line 54, forgetting "+=" expressions would make the result "incorrect",
  and not just "useless", right?

- page 5, line 50
  Remark: related work by Charguéraud and Pottier argues that, to obtain a measure
  of the asymptotic amount of work performed, it is sufficient to count 1 tick
  at the entry point of every function; maybe this is simpler than also counting
  variable lookup and case-dispatch?

- p6, line 18
  Carrying costs all the way down to a VM may bring some improvement, yet the story
  does not end there, since the hardware operations have costs that are quite hard
  to model.

- p6, line 30
  Wouldn't it make your life simpler if you were to extract code before you
  instrument it using your monad? I agree that the level of trust might slightly
  decrease, but the code wouldn't be cluttered so much.

- p6 line 56
  May I call "P" a "post-condition" (characterizing both the result value and the
  evaluation cost)?

- p7, line 22
  If you use "implicit arguments", the type "(@bin_tree A)" shouldn't show up.
  But maybe it is better to be explicit for the sake of presentation?

- p8, line 22
  Please perform a renaming to avoid shadowing the name "x"; that would make
  the definition easier to parse.

- p10, line 42
  I was suprized that fib is O(n^2) and not O(n). It is only explained why much
  later, when discussing the cost of arithmetic operations.

- p13, line 44
  The definition of copy_log_sq_time follows directly from the code. It seems
  that it could be generated automatically. Could your auxiliary tool that
  instruments the code with the monad generate the definition of the cost function
  at the same time? That would be very handy, I believe.

- p21, line 20, in relation to Fig6; and same applies to Fig7.
  Reading: "The plot suggests that primitive operations used by diff require only
  amortized constant time and suggest that a proof of this claim should be possible."
  This is far too much extrapolation in my opinion. My suggestion: remove the plot
  and write a weaker claim, along the line of "Experimental evaluation suggests that
  the cost might be linear time; We leave the complexity analysis and its formalization
  in Coq to future work."

- p23, line 22
  It would be useful to have at least one sentence to explain what Crary and Weirich's
  work achieves, before comparing. Same for Danielsson's work, it would be useful to
  explain very briefly the kind of monad involved.

- p23, line 40
  "They use an external approach, ..., which makes the proofs more complex".
  I dislike the (nearly-religious) judgment that an external approach is inherently
  more complex than putting all information in the types. My suggestion: simply
  explain the difference between an external and a dependently-typed approach.
  Both have pros and cons, and both are interesting to study. There is no need
  to try to claim than one is better than the other.

- p23, line 46
  You write that Chargueraud and Pottier's work does not consider running time.
  Yet, they establish asymptotic bounds on the number of function calls involved in
  the execution of the programs. This seems to me equivalent to the type of bounds
  that are achieved in the present paper. Thus, I am curious as to what was meant there...
  Regarding characteristic formulae, they don't really "produce" proof obligations.
  It is more like an external approach, where one first write the code (in OCaml,
  not in Coq), and then one can reason in Coq about the behavior of the code and the time
  resources that it consumes after the fact. Time credits are expressed as linear
  predicates in Separation Logic, written $n. Each time that the reasoning reaches
  the first line of a function body, there is a logical equivalent of the "+= 1",
  which can be traversed only by consuming $1. The total number of dollars available
  in the initial pre-condition reflects the total cost. It is thus somewhat the dual
  of your approach, where the post-condition, and not the pre-condition, carries
  the total cost.

- p23, line 50
  "These approaches are weaker than our approach" => do you mean weaker in the
  sense "less expressive"? If so, please clarify.

- p24, line 22
  You mention interest in extensions to support imperative programs. Note that
  Chargueraud and Pottier's work does support imperative code.








Reviewer #3: In this paper the authors implement a monad in which computations carry a resource
consumption counter. This monad is implemented in a Coq library. In addition,
they provide a transformation that interleaves monadic actions in function
definitions written in Coq. These actions basically increment the resource
consumption counter according to a predefined cost model. When extracting the Coq
specification into an OCaml program these actions are removed, since they are
irrelevant to the actual computation. The authors provide several examples involving
the usual algorithms and data structures, and justify the chosen cost model in
terms of the amortised cost of basic operations.

The main additions with respect to the FLOPS'16 paper are:

* A table with detailed figures about the length of the proofs of each case
  study.

* Further justification about the constant/linear time assumption of some
  numeric primitives.

The paper is neatly written and fairly easy to read. However, I find a few flaws
in the structure of the last part. Sections 7 and 8 actually address the same
topic (the cost of primitives), so its division in two sections seems
unnatural. Even if section 8 is the original contribution w.r.t. FLOPS'16, its
contents should be merged with those of section 7.

As regards the overall contributions of the paper, I think they represent
a significant and practical advance with respect to previous work, such as
Danielsson (2008). It also takes the cost of basic primitives into account,
which is something that many related analyses disregard by assuming constant
costs. Unfortunately, the results of section 8 are still somewhat preliminary.
The proofs given in this section look rather hand-wavy, and some others are
just missing. It is also unclear whether these proofs have been formally verified
in Coq. I suggest reworking sections 7 and 8 so as to give more formal
statements of what it is being proved.

There exist another already studied approach for accounting for language
primitives [Jost 2009, Aspinall 2007], which consists in assigning a symbolic
constant to each of these primitives, so that the cost of the overall algorithm
is expressed in terms of these constants. The latter can be subsequently replaced by
numeric constants when the cost of the primitives is determined. In this case,
those symbolic constants would be symbolic functions, as the cost of their
corresponding primitives depends on their input, and the proofs would probably
involve properties about summations. Have you explored this alternative?

Another advantage of this approach is that it can handle several size models
for expressing the cost of a given function. For instance, if we are given
a function on Braun trees whose cost depends on both the size and the height of the
input tree, we can extend the Braun predicate in order to include another
parameter representing the height of the tree, which allows one to formulate
a cost function on both parameters (size and height).

In the paper it is said that the same approach can be applied to several
resources, provided there is a mapping from costs to program constructs. However,
I do not share the authors' optimism. The monad can deal with those resources
that are compositionally additive, such as time, number of calls to a given
function, etc. There are, however, other kinds of resources (such as live heap
consumption) that, given a program, cannot be simply expressed as the sum of the
costs of the subcomponents of the program. Estimating heap and stack consumption
is a different problem which has already been studied in the context of static
analysis [Albert 2013, Montenegro 2014].

Specific comments:
------------------

- Page 2: "... @bin_tree A ..."

  Please specify what does the initial '@' mean.

- Page 3, Figure 1: "... fl_log n ..."

  Is the value of fl_log 0 defined, as in the case of the empty tree?

- Page 5, last line. "(add-plusses/check-stx-errs in rkt/tmonad/main.rkt)"

  The definition of check-stx-errs is missing from main.rkt, and also from
  private/the-lang.rkt.

- Page 9, lines -2 and -3. "one proves that (n_m + (n_f + n_g)) is an accurate
  prediction of running time and then proves that ((n_m + n_f) + n_g) is
  an accurate prediction of running time"

  This is one of the main issues you have to deal with when extending your
  approach to non-additive resources (for instance, memory consumption), where
  both + and `max` are involved.

- Page 9, line -1: "equivalant" --> "equivalent"

- Page 10, sec. 6.1.

  Proving time bounds should involve arithmetic reasoning. Did you need a
  specific Coq tactic to solve the arithmetic goals, or could you manage well
  with the standard ones provided by Coq (omega, etc.)?

- Pages 13 and 14. "However, for most of the functions the running time is
  first expressed precisely in a manner that matches the structure of the
  function and then that running time is proven to correspond to some asymptotic
  complexity, as with copy_log_sq"

  In this case it seems easy to discharge the proof obligations of the program
  whose complexity is being proved, as the time cost function matches the
  structure of the function. I mean, it should be easy to prove that the
  cost of executing "copy_log_sq n" is "copy_log_sq_time n". Would it be
  feasible to perform the proof, or generate the proof, in a mechanical way?

- Page 20. Section 8.2, on the addition "lsize + rsize + 1"

  You have already explained this in Page 16.

- Page 21. "The plot suggests that primitive operations used by diff require only
  amortised constant time..."

  I am not so sure about this, from what I can see in Figure 6. There is an
  ascending sequence containing the topmost points at x=128, 256, 512, 1024, ...
  That curve looks a bit "logarithmic" to me. Perhaps you should take a longer
  interval in the sizes of the argument to diff, in order to make clear that it
  is actually a constant.

- Section 9. "Charguéraud (2010) and Charguéraud and Pottier (2015)'s ...
        but it does not consider running time"

  In [de Dios, Peña 2011] the authors deal with the mechanical generation of
  proofs on space complexity.

References:
----------

S. Jost et al. Carbon Credits for Resource-Bounded Computations Using
Amortised Analysis. FM 2009. LNCS 5850. Springer (2009)

D. Aspinall et al. A Program Logic for Resources. Journal of Theoretical Computer
Science, vol 389(3). Elsevier (2007)

E. Albert et al. Heap space analysis for garbage collected languages. Science of
Computer Programming, vol 78. Elsevier (2013).

M. Montenegro et al. Space consumption analysis by abstract interpretation: Inference
of recursive functions. Science of Computer Programming, vol 111. Elsevier (2014).

J. de Dios, R. Peña. Certification of safe polynomial memory bounds. Formal
Methods 2011. LNCS 6664. Springer (2011).
